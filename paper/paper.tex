\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english,ngerman]{babel}
\usepackage{microtype}
\usepackage[]{csquotes}
\usepackage[top=60pt,bottom=60pt,left=80pt,right=80pt]{geometry}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}
\usepackage{pgfplots}


\title{Dateisystem Benchmark: Btrfs \& F2FS}
\author{Sven Fiergolla \& Tobias Dahlem\\
Betriebssysteme WiSe 2019/2020\\
Universität Trier}
\date{3. März 2020}


\begin{document}

\maketitle

\section{Flashspeicher}
\label{flash}
Flashspeicher werden heutzutage in vielen Bereichen verwendet, wie Computern, Smartphones und Servern. Neben ihren Vorteilen, wie die schnellere Geschwindigkeit, gibt es aber auch einige Einschränkungen, wie das Bedürfnis die Zellen zu löschen bevor sie beschrieben werden oder dass die einzelnen Blöcke nur eine beschränkte Lebensdauer besitzen. In dem Bereich der Dateisysteme gibt es verschiedene Vertreter, welche die Eigenschaften der Flashspeicher berücksichtigen. Zu den durch die Bauart bestimmten Charakteristika zählen das Adress Mapping, die Garbage Collection und das Wear Leveling \cite{lee2015f2fs}.

Im Rahmen des Adress-Mappings erfolgt die zuordnung der logischen Adressen der Speicherblöcken zu den physischen. Durch die Besonderheit des Flashspeichers, wie zum Beispiel dem Bedürfnis eine Zelle zu löschen bevor sie geschrieben werden kann, kann dieses Mapping häufig geändert werden.
Die Besonderheit der Garbage Collection ist, dass je nach Granularität in den löschbaren Einheit der Speicherblöcke mehrere Dateien gespeichert sein können. Nach der Änderung eines Teils der Daten aus einem Block, können also bestimmte Teile veraltet sein. Im Rahmen der Garbage Collection liegt also der Fokus darauf, veraltete Daten effizient auszusortieren ohne die I/O-Performance zu verschlechtern, um nicht unnötig Speicherplatz zu verbrauchen.
Im Rahmen des Wear-Levelings wird versucht den Verschleiß aller Speicherzellen auf einem gleichmäßigem Stand zu halten, bedingt durch die beschränkte Haltbarkeit dieser Zellen \cite{CHUNG2009332}.

Die meisten Dateisysteme werden dabei nicht direkt auf Flashspeicher zugeschnitten und führen diese Aufgaben nicht selbst aus, sondern verwenden dafür als Schnittstelle den sogenannten File Translation Layer (FTL) der die zuvor erwähnten Aufgaben umsetzt und dem Dateisystem eine definierte Schnittstelle zur Verfügung stellt ohne den Zugriff auf die \enquote{raw} Speicherzellen zu gewähren \cite{lee2015f2fs}.
Im folgenden Stellen wir zwei aktuelle Dateisysteme vor und Evaluieren ihre Performance in einem Benchmark-Test. Zu den Dateisystemen zählen das Flash-Dateisystem F2FS und dem Copy-On-Write Dateisystem Btrfs.


\section{Btrfs}
\label{Btrfs}
\par{
Btrfs (B-tree FS) ist ein Copy-On-Write-Dateisystem (CoW), das seit 2007 für Linux und seit 2016, mit WinBtrfs für Windows 7 entwickelt wird. Die Kernstruktur von Btrfs ist ein Copy-on-write B-Baum und wurde ursprünglich von dem IBM-Forscher Ohad Rodeh auf der USENIX 2007 vorgestellt \cite{rodeh2008b}. Zusammen mit Chris Mason \cite{mason2007btrfs}, ein ehemaliger ReiserFS Entwickler half bei der Arbeit an dem Dateisystem welches fast ausschließlich solche B-Bäume verwendet, für Meta- und Nutzdateien sowie rekursiv zur Verfolgung der Speicherzuteilung der Bäume selber\cite{rodeh2013btrfs}. Seit 2009 ist Btrfs teil des Linux-Kernel und ist das Standard Dateisystem von MeeGo und sollte seit mehreren Versionen auch in Fedora als Standard Dateisystem verwendet werden, jedoch gab Red Hat 2017 bekannt, Btrfs nicht mehr zu unterstützen und langfristig zu entfernen.
}
\par{
Btrfs weist zahlreiche Gemeinsamkeiten mit ZFS auf, welches jedoch wegen seines Lizenzstatus für die Verwendung im Linux Kernel ungeeignet war. Beide Dyteisysteme basieren auf \emph{Copy-On-Write}, wobei eine Kopie erst dann angefertigt wird, wenn sie wird. Zudem implementieren sie beide, Volume-Management, Schutz vor Datenübertragungsfehlern durch Prüfsummen und integriertes RAID. Btrfs baut mit dem B-Baum auf einem zentralen Konzept von XFS oder Reiser4 auf. Mit btrfs-convert können bestehende ext3- und ext4- Dateisysteme in ein Btrfs Dateisystem konvertiert werden, sowie umgekehrt. Weitere Features sind der erweiterte Speicherbereich (264 Byte), effizientes Speichern kleiner Dateien und Verzeichnisse, dynamische Inodes
Snapshots, Subvolumes und iDatenkompression auf Dateisystemebene.}
\par{
Das von btrfs implementierte RAID bietet gegenüber klassischen Hardware- oder Software-RAID Implementierungen den Vorteil, dass zwischen belegten und freien Datenblöcken unterschieden werden kann und somit bei der Rekonstruktion eines gespiegelten RAID-Volumens nur belegter Plattenplatz gespiegelt werden muss. Im Gegensatz zu klassischen RAID-Verfahren wird mit Hilfe von größeren Datenblöcken gearbeitet und es erfolgt beispielsweise im RAID 1 keine Spiegelung der Datenträger, sondern es wird sichergestellt, dass jeder Datenbereich auf wenigstens zwei Datenträgern abgelegt wird. So wird es möglich einen RAID 1 aus einer ungeraden Anzahl von Datenträgern unterschiedlicher Kapazität, unter voller Ausnutzung derer Kapazität, zu bilden. Kommt es unter Nutzung des RAID 1 zu \enquote{Bitrot} oder sonstigem schleichenden Verlust von Informationen, fällt dies dank dem Einsatz von Prüfsummen beim folgenden Zugriff auf und der Fehlerhafte Block wird verworfen und durch den gespiegelten Datenbereich eretzt. Btrfs bietet ebenfalls eine RAID 0 Implementierung, welche für sich für den Einsatz von nicht kritischen Daten eignet, da dennoch mehrere Disks zu einem Eigenständigen logischen Volume zusammengefügt werden können für sehr größe Dateien. Zudem bieten sie eine deutlich gesteigerte Geschwindigkeit, vorallem beim sequentiellen Lesen, jedoch auch bei den wahlfreien Zugriffen wie in Abbildung \ref{btrfs-raid} ersichtlich ist.}

\newpage
\section{F2FS}
\label{F2FS}
F2FS ist ein Dateisystem speziell nur für Flashspeicher und wurde von Samsung 2012 veröffentlicht. Entwickelt wurde das Dateisystem mit dem Ziel der Optimierung von Performance und Lebenszeit von Flashspeichern, indem die speziellen Charakteristika berücksichtigt wurden. Dabei wurde das System für Linux als Open-Source Projekt entwickelt und ist seit dem Kernel 3.8 verwendbar, klassische Anwendungsbereiche stellen SD-Karten, embedded multimedia cards (eMMC) und klassische SSDs dar. Unter dem Android-System findet es zum Beispiel Verwendung in Smartphons von Huawei und dem Galaxy Note 10 von Samsung. Im Vergleich zu anderen Flash-Dateisystemen, kann F2FS allerdings nicht direkt auf den ,,raw''-Speicherzellen arbeiten, sondern nutzt dafür den FTL des Speichers. Im Gesamten verfügt F2FS über viele Möglichkeiten das System an die benötigt Einsatzumgebung anzupassen. Grundlegend wird dabei das UNIX-Prinzip der Inodes und Datenblöcke zur Speicherung verwendet, zusätzlich wird dabei noch der Ansatz eines Log-structured File Systems (LFS), im Sinne eines append-only loggings, verfolgt \cite{lee2015f2fs}.

Im Folgenden werden einige Besonderheiten des F2FS Dateisystems aufgezeigt, welche das System charakterisieren, dazu zählen das On-Disk-Layout, das Multi-Head Logging, die Indexstruktur und die Garbage Collection.

Das On-Disk-Layout orientiert sich von der Blockgröße an der Größe von 4KB. Im Gesamten werden aller Speicherzellen in Blöcke dieser Größe unterteilt. Die weiteren übergeordneten Einheiten sind Segmente, Sections und Zonen. Im Gesamten ist das Volumen in 2 Bereiche aufgeteilt, die Metadaten und der Hauptbereich. Zu den Metadaten zählt der Superblock mit allgemeinen Informationen über das Dateisystem.
Die Checkpoints geben im Rahmen des LFS validen Zustände an.
%des im Rahmen des LFS werden in einem einzelnen Bereich im Prinzip einer Schattentechnik gespeichert.
Die Segment-Information-Table wird für jedes Segment gespeichert, welche beinhalteten Blöcke noch gültig sind oder ob er im Rahmen der Garbage Collection gereinigt werden könnte.
In der Segment Summary Area werden sogenannte Informationen für alle Blöcke eines Segments gesichert, sie beinhalten Informationen wie zu welcher Datei jeder Block gehört.
Die Node-Address-Table ist eine Block-Adressen-Tabelle, welche für die physischen Adressen jedes Speicherblocks eine logischen Adresse kennt. Durch diese Tabelle wird das sogenannte wandernde Baum Problem, welches beim Ändern von Daten entsteht, bekämpft. Verweise in Blöcken zeigen in F2FS daher nicht auf die physischen Blöcke, sondern speichern eine logische Adresse und erfragen die physische über die NAT. So müssen bei der Adressänderung eines Blocks nicht mehr alle Verweise angepasst werden, was einen hohen Schreibaufwand bedeutet, sondern nur die Zuordnung innerhalb der NAT. Zusätzlich wird der Metadaten-Teil nicht im Sinne eines LFS verwaltet, sondern In-Place im Rahmen einer Schattenkopie. Um den Schreibaufwand bei diesen Random-Writes auf bestimmten Speicherzellen nicht zu überlasten, werden häufig geschriebene Daten wie die NAT in einen flüchtigen Speicher gehalten und nur zum setzten eines Checkpoints auf die veraltete Schattenkopie geschrieben \cite{lee2015f2fs}.

Im Hauptspeicher sind die iNodes und Datenblöcke abgelegt, dabei wird hier im Sinne eines LFS vorgegangen. Allerdings liegen diese in verschiedenen Bereichen, denn F2FS verwendet den Ansatz eines Multi-Head Loggings und hat sechs parallele Logsegmente, deren parallel Verwendung durch die klassische Flash-Architektur möglich ist. Die Idee dabei ist, die Daten, iNodes und Datenblöcke, in Klassen hot/warm/cold zu klassifizieren, in Bezug auf ihre Update-Frequenz. Dadurch soll ein häufiges neu schreiben von validen Teilsegmenten vermieden werden, wenn nur Teil des Segments veraltet sind. Ebenfalls wird dadurch die Anzahl der veralteten Daten reduziert und die Garbage Collection vereinfacht \cite{lee2015f2fs}.

Die Garbage Collection (GC) wird auf zwei Arten durchgeführt, On-Demand und im Hintergrund. On-Demand wird dabei verwendet, wenn nicht genügend Speicherplatz zum schreiben verfügbar ist. In diesem Sinne wird dann, mit einem Greedy Algorithmus, auf Basis der wenigsten gültigen Blöcken ein Opfersegment ausgesucht. Im Hintergrund wird die GC mit einen Kosten-Effizienten Algorithmus bei einer geringen Auslastung des Dateisystems automatisch ausgeführt. Dieser wählt Opfersegmente auf Grund ihres Alters und der Anzahl der gültigen Blöcke aus. Die Blöcke, welche noch gültig sind, werden dann wie bei einem normalen Schreiben wieder in die aktiven Segmente eingereiht und im Anschluss geschrieben. Die Besonderheit hierbei ist, dass wieder eine neue bessere Klassifizierung des hot/warm/cold-Schemas vorgenommen werden kann, da nun mehr Informationen über die Blöcke vorhanden sind \cite{lee2015f2fs}.


\section{Benchmarks}
	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[
		width=\textwidth,
		height=0.45\textwidth,
		axis x line=center,
		axis y line=left,
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		ylabel={MB/s},
		symbolic x coords={HDD 7200rpm, HDD 5400rpm, USB flash dive,flash SD Card},
		x tick label style={font=\small,text width=1cm,align=center},
		xtick=data,
		enlargelimits=true,
		legend style={
			at={(0.5,1.2)},
			anchor=north,
			legend columns=-1,
			/tikz/every even column/.append style={column sep=0.5cm}
		},	
		ymajorgrids=true,
		grid style=dashed,
		ybar
		]
		\addplot[color=blue,fill]
		coordinates {(HDD 7200rpm, 27.7)(HDD 5400rpm, 18.3)(USB flash dive, 178)(flash SD Card,21.9)};
		\addplot[color=red,fill]
		coordinates {(HDD 7200rpm, 50)(HDD 5400rpm, 46.1)(USB flash dive, 18.6)(flash SD Card,10.9)};
		\addplot[color=black,fill]
		coordinates {(HDD 7200rpm, 8.7)(HDD 5400rpm, 9.7)(USB flash dive, 143)(flash SD Card, 11.6)};
		\addplot[color=black]
		coordinates {(HDD 7200rpm, 13.4)(HDD 5400rpm, 10.3)(USB flash dive, 15.8)(flash SD Card, 0.9)};
		\legend{seq read, seq write, random read, random write}
		\end{axis}
		\end{tikzpicture}
		\label{f2fs-raw}
		\caption{F2FS raw performance}
	\end{figure}

	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[
		width=\textwidth,
		height=0.45\textwidth,
		axis x line=center,
		axis y line=left,
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		ylabel={MB/s},
		symbolic x coords={HDD 7200rpm,HDD 5400rpm,USB flash dive},
		x tick label style={font=\small,text width=1cm,align=center},
		xtick=data,
		enlargelimits=true,
		legend style={
			at={(0.5,1.2)},
			anchor=north,
			legend columns=-1,
			/tikz/every even column/.append style={column sep=0.5cm}
		},	
		ymajorgrids=true,
		grid style=dashed,
		ybar
		]
		\addplot[color=blue,fill]
		coordinates {(HDD 7200rpm, 91)(HDD 5400rpm,80)(USB flash dive,112)};
		\addplot[color=red,fill]
		coordinates {(HDD 7200rpm, 84)(HDD 5400rpm, 73.1)(USB flash dive, 19.5)};
		\addplot[color=black,fill]
		coordinates {(HDD 7200rpm, 10.5)(HDD 5400rpm, 9.1)(USB flash dive, 116)};
		\addplot[color=black,]
		coordinates {(HDD 7200rpm, 23.5)(HDD 5400rpm, 18.7)(USB flash dive, 18.6)};
		\legend{seq read, seq write, random read, random write}
		\end{axis}
		\end{tikzpicture}
		\label{btrfs-raw}
		\caption{BtrFS raw performance}
	\end{figure}

	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[
		width=\textwidth,
		height=0.45\textwidth,
		axis x line=center,
		axis y line=left,
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		ylabel={MB/s},
		symbolic x coords={RAID 0, RAID 1, RAID 5},
		x tick label style={font=\small,text width=1cm,align=center},
		xtick=data,
		enlargelimits=true,
		legend style={
			at={(0.5,1.2)},
			anchor=north,
			legend columns=-1,
			/tikz/every even column/.append style={column sep=0.5cm}
		},	
		ymajorgrids=true,
		grid style=dashed,
		ybar
		]
		\addplot[color=blue,fill]
		coordinates {(RAID 0, 289)(RAID 1,17.6 )(RAID 5,50.4)};
		\addplot[color=red,fill]
		coordinates {(RAID 0, 54 )(RAID 1, 30.8)(RAID 5, 18.7)};
		\addplot[color=black,fill]
		coordinates {(RAID 0, 58.8)(RAID 1, 18.6)(RAID 5, 23.2)};
		\addplot[color=black]
		coordinates {(RAID 0, 40)(RAID 1, 15.5)(RAID 5, 10.3)};
		\legend{seq read, seq write, random read, random write}
		\end{axis}
		\end{tikzpicture}
				\label{btrfs-raid}
		\caption{BtrFS RAID performance}
	\end{figure}

	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[
		width=\textwidth,
		height=0.45\textwidth,
		axis x line=center,
		axis y line=left,
		x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
		ylabel={MB/s},
		symbolic x coords={f2fs flash drive,btrfs flash drive,btrfs RAID 0},
		x tick label style={font=\small,text width=1cm,align=center},
		xtick=data,
		enlargelimits=true,
		legend style={
			at={(0.5,1.2)},
			anchor=north,
			legend columns=-1,
			/tikz/every even column/.append style={column sep=0.5cm}
		},	
		ymajorgrids=true,
		grid style=dashed,
		ybar
		]
		\addplot[color=blue,fill]
		coordinates {(f2fs flash drive, 178)(btrfs flash drive,112)(btrfs RAID 0, 289)};
		\addplot[color=red,fill]
		coordinates {(f2fs flash drive, 18.6)(btrfs flash drive, 19.5)(btrfs RAID 0, 54)};
		\addplot[color=black,fill]
		coordinates {(f2fs flash drive, 143)(btrfs flash drive, 116)(btrfs RAID 0, 58)};
		\addplot[color=black,]
		coordinates {(f2fs flash drive, 15.8)(btrfs flash drive, 18.6)(btrfs RAID 0,40)};
		\legend{seq read, seq write, random read, random write}
		\end{axis}
		\end{tikzpicture}
				\label{f2fs-vs-btrfs}
		\caption{F2FS vs. BtrFS vs. BtrFS RAID0}
	\end{figure}

\bibliographystyle{plain}
\bibliography{thesis}
\end{document}
